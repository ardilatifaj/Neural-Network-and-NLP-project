{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc0ee89",
   "metadata": {},
   "source": [
    "# Online Reviews Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70cc05",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Introduction and Research Question](#1-bullet)\n",
    "* [Dataset](#2-bullet)\n",
    "* [Load and Clean the Data](#3-bullet)\n",
    "* [Descriptive Statistics](#4-bullet)\n",
    "* [Preprocessing](#5-bullet)\n",
    "* [Neural Network](#6-bullet)\n",
    "* [Sources](#11-bullet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582210e1",
   "metadata": {},
   "source": [
    "# Introduction and research question <a class=\"anchor\" id=\"1-bullet\"></a>\n",
    "\n",
    "As online shopping is one of the most popular online activities worldwide (NTIA 2019), the product reviews and ratings have become of significant relevance. In a survey with 2,000 consumers, conducted in the U.S., 93% of the respondents claim that reviews influenced their purchase decision (Podium, 2018). The same study reveals that two-thirds of the consumers would be able to pay 15% more for the same product if they were told that the product has a good customer experience.\n",
    "\n",
    "In this project, we examine the suitability of Neural Networks (NN) for the classification of reviews as positive or negative. To see how well NN perform, we compare it with the Multinomial Naive Bayes Classifier. We identify whether the review for the product is positive or negative based on the reviews and ratings provided by other customers. This helps to analyze productsâ€™ reviews on platforms that do not have a rating option - such as Twitter or other social media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617f729",
   "metadata": {},
   "source": [
    "# Dataset <a class=\"anchor\" id=\"2-bullet\"></a>\n",
    "\n",
    "For the project, we use a datasets that contains Amazon reviews with ratings, which we take from kaggle.com. The training data contains 3,600,000 reviews whereas the testing data contains 400,000 reviews in total. The ratings in the dataset are presented in the following way: one- and two-star ratings have the value 1, whereas four- and five-star ratings have the value 2. The three-star ratings are not included in the dataset, because they do not help to classify the reviews as positive or negativ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8d0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from nltk.stem import PorterStemmer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369928b8",
   "metadata": {},
   "source": [
    "# Load and Clean the data <a class=\"anchor\" id=\"3-bullet\"></a>\n",
    "\n",
    "We used the function load_data to load the raw data into a pandas dataframe. In the data cleaning process, we first define the path of the dataset, in which we splite the data, rename columns to label and text, change the label from string to number and change the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502fff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path: path of dataset\n",
    "# max_rows: maximum number of rows\n",
    "\n",
    "def load_data(data_path, max_rows):\n",
    "    # read data\n",
    "    df = pd.read_csv(data_path,sep='\\t', header=None)\n",
    "    # split data in label and text\n",
    "    df = df[0].str.split(' ', n=1, expand=True)\n",
    "    # rename columns\n",
    "    df = df.rename(columns={0: \"label\", 1: \"text\"})\n",
    "    # change label form string to number\n",
    "    df['label'] = df['label'].str[-1]\n",
    "    # change number of rows\n",
    "    if df.shape[0]> max_rows:\n",
    "        df = df.iloc[0:max_rows]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6804b73",
   "metadata": {},
   "source": [
    "Having performed this step, we defined the length of the training and test dataset. For the training dataset, we used 360,000 reviews, whereas for the testing dataset we used 40,000 reviews. We decided to shrink the number of reviews and ratings to quickly run and compare different models. The possible consequences of shrinking the datasets will be also discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8ce333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lengths of training and testing data\n",
    "\n",
    "n_train = 360000\n",
    "n_test = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eddcbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing data\n",
    "\n",
    "df_train = load_data('data/train.txt', n_train)\n",
    "df_test = load_data('data/test.txt', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98abd661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0     2  Stuning even for the non-gamer: This sound tra...\n",
       "1     2  The best soundtrack ever to anything.: I'm rea...\n",
       "2     2  Amazing!: This soundtrack is my favorite music...\n",
       "3     2  Excellent Soundtrack: I truly like this soundt...\n",
       "4     2  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show training data\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b929ab",
   "metadata": {},
   "source": [
    "# Descriptive Statistics<a class=\"anchor\" id=\"4-bullet\"></a>\n",
    "\n",
    "Once we loaded the data, we observed the label and text columns in more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcdbe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     360000\n",
       "unique         2\n",
       "top            2\n",
       "freq      182351\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratings\n",
    "\n",
    "df_train['label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b99f7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  len\n",
       "0     2  Stuning even for the non-gamer: This sound tra...  426\n",
       "1     2  The best soundtrack ever to anything.: I'm rea...  509\n",
       "2     2  Amazing!: This soundtrack is my favorite music...  760\n",
       "3     2  Excellent Soundtrack: I truly like this soundt...  743\n",
       "4     2  Remember, Pull Your Jaw Off The Floor After He...  481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include the numer of characters of the texts in the dataframe\n",
    "\n",
    "df_train['len'] = df_train['text'].str.len()\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036b50a",
   "metadata": {},
   "source": [
    "We also included the length of the text in the data frame. In the histogram given below, the distribution of review length can be observed. The average length of the train data reviews is 440 characters, whereas the maximum and minimum lengths are 1015 and 97, respectively. We notice that negative reviews are slightly longer than the positive ones (456 and 423, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e532e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    360000.000000\n",
       "mean        439.802650\n",
       "std         239.861798\n",
       "min          97.000000\n",
       "25%         237.000000\n",
       "50%         393.000000\n",
       "75%         608.000000\n",
       "max        1015.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze the lenth of the texts\n",
    "\n",
    "df_train['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8dcbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot distribution of the lenght of the reviews\n",
    "\n",
    "plt.hist(df_train['len'], bins=100)  # density=False would make counts\n",
    "plt.ylabel('number')\n",
    "plt.xlabel('length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2971cdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>456.136224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423.890245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              len\n",
       "label            \n",
       "1      456.136224\n",
       "2      423.890245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare length of revies by rating\n",
    "\n",
    "df_train.groupby(['label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9e5d9f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237.500182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241.079158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              len\n",
       "label            \n",
       "1      237.500182\n",
       "2      241.079158"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard deviation\n",
    "\n",
    "df_train.groupby(['label']).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c88155",
   "metadata": {},
   "source": [
    "# Preprocessing<a class=\"anchor\" id=\"5-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffa23c",
   "metadata": {},
   "source": [
    "In this section, we prepared and cleaned the text data and made it ready to feed it to the model. The transform_data method takes a dataframe, the ngram specification, a list of words or the maximum number of features for which the matrix of token counts should be created. If the list of words vocab is set to None, max_feat is used, else the list of words is created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8e43889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, ngram, vocab, max_feat):\n",
    "    \n",
    "    # tokenization\n",
    "    rx = '[a-zA-Z]{3,20}'\n",
    "    cv = CountVectorizer(vocabulary=vocab, ngram_range=ngram, max_df=0.95, min_df=3, max_features=max_feat, stop_words='english', token_pattern = rx)\n",
    "    cvf = cv.fit_transform(df['text'])\n",
    "    words = cv.get_feature_names()\n",
    "    \n",
    "    # word matrix\n",
    "    count_vect_df = pd.DataFrame(cvf.todense(), columns=cv.get_feature_names())\n",
    "    \n",
    "    # input and output data\n",
    "    x = count_vect_df.to_numpy()\n",
    "    y = df['label'].to_numpy().astype(int)\n",
    "    # change 1 to 0 and 2 to 1\n",
    "    y = np.where(y==1, 0, 1)\n",
    "    \n",
    "    # results\n",
    "    output = [words, x, y]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f6ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maximum number of features\n",
    "\n",
    "max_feat = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decac6ec",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eff20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training data\n",
    "# we set vocab to None in order to automatically create a list of words\n",
    "\n",
    "train_data = transform_data(df_train,(1,1), None, max_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56a300",
   "metadata": {},
   "source": [
    "After performing the transform_data method for the training data, we obtained a list with three elements. The first one consists of a list with the 100 most frequent words, because we set the vovabulary to None - this created automatically a list of most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ea12c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually',\n",
       " 'album',\n",
       " 'amazon',\n",
       " 'author',\n",
       " 'bad',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boring',\n",
       " 'bought',\n",
       " 'buy',\n",
       " 'characters',\n",
       " 'come',\n",
       " 'day',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'different',\n",
       " 'disappointed',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'dvd',\n",
       " 'easy',\n",
       " 'end',\n",
       " 'excellent',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'feel',\n",
       " 'film',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'hard',\n",
       " 'interesting',\n",
       " 'just',\n",
       " 'know',\n",
       " 'life',\n",
       " 'like',\n",
       " 'little',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'money',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'music',\n",
       " 'need',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'old',\n",
       " 'people',\n",
       " 'plot',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'product',\n",
       " 'quality',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'really',\n",
       " 'recommend',\n",
       " 'reviews',\n",
       " 'right',\n",
       " 'say',\n",
       " 'series',\n",
       " 'set',\n",
       " 'song',\n",
       " 'songs',\n",
       " 'sound',\n",
       " 'story',\n",
       " 'sure',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'use',\n",
       " 'used',\n",
       " 'version',\n",
       " 'want',\n",
       " 'waste',\n",
       " 'watch',\n",
       " 'way',\n",
       " 'work',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'written',\n",
       " 'year',\n",
       " 'years']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words\n",
    "\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647d8f8",
   "metadata": {},
   "source": [
    "The second consists of a vector with a length of the entire vocabulary â€“ namely, 100 words - and an integer count for the number of times each word appeared in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ff563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word-count-matrix\n",
    "\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714a1da",
   "metadata": {},
   "source": [
    "The third and final output, consists of a single vector of ratings with values 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a0971f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratings\n",
    "\n",
    "train_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318eae8d",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "We applied the same function to the test dataset and used the list of words created for the training data such that the list of words for the test dataset is the same list as one for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test dataset\n",
    "# we set vocab to the automatically created wordlist of the traning dataset \"train_data[0]\"\n",
    "\n",
    "test_data = transform_data(df_test,(1,1), train_data[0], max_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fea00",
   "metadata": {},
   "source": [
    "# Neural Network<a class=\"anchor\" id=\"6-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5177597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for NN\n",
    "\n",
    "num_classes = None\n",
    "\n",
    "x_train = train_data[1].reshape(360000, max_feat)\n",
    "x_test = test_data[1].reshape(40000, max_feat)\n",
    "y_train = train_data[2].reshape(360000, 1)\n",
    "y_test = test_data[2].reshape(40000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d15864d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,401\n",
      "Trainable params: 30,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build NN\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(max_feat, activation='relu', input_shape=(max_feat,)))\n",
    "model.add(Dense(100, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(100, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc399db5",
   "metadata": {},
   "source": [
    "In the next step, the model was trained based on the defined parameters and afterwards evaluated. As you can see below, the validation loss was about 49% and the validation accuracy was close to 76%, which is quite close to the results of the model which indicates, for example, that the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb33f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "11250/11250 [==============================] - 10s 844us/step - loss: 0.5038 - accuracy: 0.7474 - val_loss: 0.4926 - val_accuracy: 0.7559\n",
      "Epoch 2/3\n",
      "11250/11250 [==============================] - 9s 802us/step - loss: 0.4905 - accuracy: 0.7564 - val_loss: 0.4899 - val_accuracy: 0.7560\n",
      "Epoch 3/3\n",
      "11250/11250 [==============================] - 9s 814us/step - loss: 0.4854 - accuracy: 0.7597 - val_loss: 0.4878 - val_accuracy: 0.7572\n",
      "Test loss: 0.48778119683265686\n",
      "Test accuracy: 0.7571750283241272\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e37864",
   "metadata": {},
   "source": [
    "Finally, we set up a function to predict whether a review is bad or good. This was done by converting the new review to a vector of token numbers and using a built-in list to exclude stop words. We then converted the matrix into a data frame, which allowed us to perform the prediction. The prediction either gives back a 0 for a bad review or a 1 for a good review. Below are three examples of bad ratings and the (correct) output of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ed8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict if a rewiew is positive of negative\n",
    "# 0: bad review\n",
    "# 1: good review\n",
    "\n",
    "def test_string(test_text, model, vocab):\n",
    "    # tokenization\n",
    "    rx = '[a-zA-Z]{3,20}'\n",
    "    cv = CountVectorizer(vocabulary=vocab, stop_words='english', token_pattern = rx)\n",
    "    cvf = cv.fit_transform([test_text])\n",
    "    # convert matrix to dataframe\n",
    "    output = pd.DataFrame(cvf.todense(), columns=cv.get_feature_names())\n",
    "    # prediction\n",
    "    x = np.argmax(model.predict(output.to_numpy()))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d428cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "text1 = \"Really impressive for Â£319. Obviously thatâ€™s still a lot of money but with the pro pretty much 3x that and most competitors higher the quality for price is brilliant. Great screen quality and super quick.\"\n",
    "text2 = \"Believed I was buying the new mobile I pad, needed up with the old model\"\n",
    "text3 = \"I agree on other comments that feet are a nightmare to get on,! Picture quality nice and sharp so no problems there however it takes forever to load. I have never had a tv so slow to load. It is very frustrating. I am about to buy another tv for another room and it will not be one of these. I would also add it is not a broadband issue. It is the tv. Update. After nearly two months this tv is getting slower and slower. It is almost impossible to use. A month ago I bought a Hisense tv for another room and it works perfectly. Avoid this JVC. It is so bad it is best classed as junk\"\n",
    "\n",
    "test_string(\"bad product\", model, train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cb342",
   "metadata": {},
   "source": [
    "# Sources <a class=\"anchor\" id=\"11-bullet\"></a>\n",
    "\n",
    "Dataset: https://www.kaggle.com/bittlingmayer/amazonreviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
